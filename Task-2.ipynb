{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "GBDT算法梳理\n",
    "\n",
    "前向分布算法\n",
    "负梯度拟合\n",
    "损失函数\n",
    "回归\n",
    "二分类，多分类\n",
    "正则化\n",
    "优缺点\n",
    "sklearn参数\n",
    "应用场景\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBDT算法  Gradient Boosting DecisionTree\n",
    "    是一种迭代的决策树算法，⼜又叫 MART(Multiple Additive Regression Tree)，\n",
    "    它通过构造⼀一组弱的学习器器(树)，并把多颗决策树的结果累加起来作为最终的预测输出。\n",
    "    该算法将决策树与集成思想进⾏了有效的结合。\n",
    "    GBDT中的树都是回归树（使用平方损失函数），⽽不是分类树，它⽤来做回归预测，经过调整之后也能⽤用来做分类。\n",
    "    集成算法是将一组弱分类器结合一起形成一个强分类器，有boosting和bagging\n",
    "    随机森林是属于bagging的使用决策树的集成算法，并行的生成N多决策树，最后投票生成结果\n",
    "    adaboost是属于boosting的也是默认使用决策树的集成算法，串行执行，对样本进行加权操作\n",
    "    在GBDT的迭代的过程中，新一轮迭代的目的就是找到⼀个弱分类器器 ，使得前一轮的损失函数达到最⼩，Freidman提出了梯度提升算法:\n",
    "        利用最速下降的近似方法，即利⽤用损失函数的负梯度在当前模型的值作为回归问题中提升算法的残差的近似值，这就是梯度提升决策树。\n",
    "    加法模型 + 前向分布算法 + CART树(基函数)。\n",
    "    \n",
    "提升树 Boosting Decision Tree\n",
    "    以决策树为基函数的提升方法，采用加法模型（即基函数的结性组合）与前向分布算法。\n",
    "    它是迭代多棵回归树来共同决策，每一轮回归树学到的是之前所有树的结论和残差，拟合得到一个当前的残差回归树，\n",
    "    是整个迭代过程生成的回归树的累加。残差的意义如公式:残差 = 真实值 - 预测值\n",
    "        \n",
    "泰勒公式\n",
    "    函数在一个点的邻域内的值可以用函数在该点的值及各阶导数值组成的无穷级数表示出来\n",
    "    是将一个在x=x0处具有n阶导数的函数f（x）利用关于（x-x0）的n次多项式来逼近函数的方法。\n",
    "    一阶展开： 梯度下降法\n",
    "    二阶展开：牛顿法\n",
    "    \n",
    "GBDT与XGBOOST eXtreme Gradient Boosting\n",
    "    GBDT在函数空间中利用梯度下降法进行优化 \n",
    "    XGBoost在函数空间中用牛顿法进行优化\n",
    "    XGBoost的目标函数多了正则项，正则项对每棵回归树的复杂度进行了惩罚，使得学习出来的模型更加不容易过拟合\n",
    "    \n",
    "树模型有以下优缺点:\n",
    "    优点\n",
    "        • 可解释性强\n",
    "        • 可处理混合类型特征\n",
    "        • 具体伸缩不变性(不用归一化特征) • 有特征组合的作用\n",
    "        • 可自然地处理缺失值\n",
    "        • 对异常点鲁棒\n",
    "        • 有特征选择作用\n",
    "        • 可扩展性强，容易并行\n",
    "    缺点\n",
    "        • 缺乏平滑性(回归预测时输出值只能 输出有限的若干种数值)\n",
    "        • 不适合处理高维稀疏数据\n",
    "\n",
    "XGBOOST\n",
    "    树节点分裂方法(Split Finding)\n",
    "    • 精确算法 遍历所有特征的所有可能的分割点，计算gain值，选取值最大的(feature，value)去分割\n",
    "    • 近似算法\n",
    "      对于每个特征，只考察分位点，减少计算复杂度，不是简单地按照样本个数进行分位，而是以二阶导数值作为权重(Weighted Quantile Sketch)\n",
    "    • 当特征出现缺失值时，XGBoost 可以学习出默认的节点分裂方向\n",
    "    • 行抽样(row sample)\n",
    "    • 列抽样(column sample) 借鉴随机森林\n",
    "    • Shrinkage(缩减)，即学习速率 将学习速率调小，迭代次数增多，有正则化作用\n",
    "    • 支持自定义损失函数(需二阶可导)\n",
    "        \n",
    "LightGBM\n",
    "    直方图算法\n",
    "        • 减小内存占用，比如离散为256个bin时，只需要8bit，节省7/8\n",
    "        • 减小了splitfinding时计算增益的计算量，从O(#data)降到O(#bins)\n",
    "    直方图差加速\n",
    "        一个叶子的直方图可以由它的父亲节点的直方图与它兄弟节点的直方图做差得到，提升一倍速度\n",
    "    • 建树过程的两种方法:Level-wise和Leaf-wise\n",
    "    • 并行优化(Optimization in parallel learning)\n",
    "    • Gradient-based One Side Sampling (GOSS)\n",
    "    • Exclusive Feature Bundling (EFB)\n",
    "            \n",
    "应用场景：\n",
    "    搜索排序、点击率预估\n",
    "    使⽤其来自动发现有效的特征特征组合，来作为LR模型中的特征，以提⾼CTR预估(Click-Through Rate Prediction)的准确性;\n",
    "    淘宝的搜索及预测业务\n",
    "    \n",
    "回归树\n",
    "    分类使用GINI系数，回归使用MSE值 来分枝\n",
    "    分裂前后的增益怎么计算? ID3算法采用信息增益 C4.5算法采用信息增益比 CART采用Gini系数\n",
    "            \n",
    "GradientBoostingRegressor(loss='ls', \n",
    "                          learning_rate=0.1, \n",
    "                          n_estimators=100, subsample=1.0, \n",
    "                          criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, \n",
    "                          min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, \n",
    "                          min_impurity_split=None, init=None, random_state=None, max_features=None, \n",
    "                          alpha=0.9, verbose=0, max_leaf_nodes=None, warm_start=False, presort='auto', \n",
    "                          validation_fraction=0.1, n_iter_no_change=None, tol=0.0001)\n",
    "            \n",
    "    loss: 即我们GBDT算法中的损失函数。\n",
    "        1>对于分类模型，有对数似然损失函数\"deviance\"和指数损失函数\"exponential\"两者输入选择。默认是对数似然损失函数\"deviance\"。\n",
    "        2>对于回归模型，有均方差\"ls\", 绝对损失\"lad\", Huber损失\"huber\"和分位数损失“quantile”。默认是均方差\"ls\"。\n",
    "    n_estimators: 默认是100，最大的弱学习器的个数，或者弱学习器的最大迭代次数\n",
    "    criterion: 样本集的切分策略"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
